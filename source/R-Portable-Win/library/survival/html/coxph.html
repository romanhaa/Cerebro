<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Fit Proportional Hazards Regression Model</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for coxph {survival}"><tr><td>coxph {survival}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>
Fit Proportional Hazards Regression Model 
</h2>

<h3>Description</h3>

<p>Fits a Cox proportional hazards regression model. 
Time dependent variables, time dependent strata, multiple events per subject, 
and other extensions are incorporated using the counting process formulation 
of Andersen and Gill. 
</p>


<h3>Usage</h3>

<pre>
coxph(formula, data=, weights, subset, 
      na.action, init, control, 
      ties=c("efron","breslow","exact"), 
      singular.ok=TRUE, robust=FALSE, 
      model=FALSE, x=FALSE, y=TRUE, tt, method, ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>formula</code></td>
<td>

<p>a formula object, with the response on the left of a <code>~</code> operator, and 
the terms on the right.  The response must be a survival object as 
returned by the <code>Surv</code> function. 
</p>
</td></tr>
<tr valign="top"><td><code>data</code></td>
<td>

<p>a data.frame in which to interpret the variables named in 
the <code>formula</code>, or in the <code>subset</code> and the <code>weights</code>
argument. 
</p>
</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>

<p>vector of case weights, see the note below.
For a thorough discussion of these see the
book by Therneau and Grambsch.
</p>
</td></tr>
<tr valign="top"><td><code>subset</code></td>
<td>

<p>expression indicating which subset of the rows of data should be used in 
the fit.    All observations are included by default. 
</p>
</td></tr>
<tr valign="top"><td><code>na.action</code></td>
<td>

<p>a missing-data filter function.  This is applied to the model.frame
after any 
subset argument has been used.  Default is <code>options()\$na.action</code>. 
</p>
</td></tr>
<tr valign="top"><td><code>init</code></td>
<td>

<p>vector of initial values of the iteration.  Default initial 
value is zero for all variables. 
</p>
</td></tr>
<tr valign="top"><td><code>control</code></td>
<td>

<p>Object of class <code><a href="coxph.control.html">coxph.control</a></code> specifying iteration limit
and other control options. Default is <code>coxph.control(...)</code>.
</p>
</td></tr>
<tr valign="top"><td><code>ties</code></td>
<td>

<p>a character string specifying the method for tie handling.  If there  
are no tied death times all the methods are equivalent. 
Nearly all Cox regression programs use the Breslow method by default, 
but not this one. 
The Efron approximation is used as the default here, it is more 
accurate when dealing with tied death times, and is as efficient 
computationally. 
The &ldquo;exact partial likelihood&rdquo; is 
equivalent to a conditional logistic model, and is appropriate when
the times are a small set of discrete values.  See further below.
</p>
</td></tr>
<tr valign="top"><td><code>singular.ok</code></td>
<td>

<p>logical value indicating how to handle collinearity in the model matrix. 
If <code>TRUE</code>, the program will automatically skip over columns of the X 
matrix that are linear combinations of earlier columns.  In this case the 
coefficients for such columns will be NA, and the variance matrix will 
contain zeros. For ancillary calculations, such as the linear
predictor, 
the missing coefficients are treated as zeros. 
</p>
</td></tr>
<tr valign="top"><td><code>robust</code></td>
<td>

<p>this argument has been deprecated, use a cluster term in the model
instead.  (The two options accomplish the same goal &ndash; creation of a
robust variance &ndash; but the second
is more flexible).
</p>
</td></tr>
<tr valign="top"><td><code>model</code></td>
<td>

<p>logical value: if <code>TRUE</code>, the model frame is returned in component
<code>model</code>. 
</p>
</td></tr>
<tr valign="top"><td><code>x</code></td>
<td>

<p>logical value: if <code>TRUE</code>, the x matrix is returned in
component <code>x</code>. 
</p>
</td></tr>
<tr valign="top"><td><code>y</code></td>
<td>

<p>logical value: if <code>TRUE</code>, the response vector is returned in
component <code>y</code>. 
</p>
</td></tr>
<tr valign="top"><td><code>tt</code></td>
<td>
<p>optional list of time-transform functions.</p>
</td></tr>
<tr valign="top"><td><code>method</code></td>
<td>
<p>alternate name for the <code>ties</code> argument.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>Other arguments will be passed to <code><a href="coxph.control.html">coxph.control</a></code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The proportional hazards model is usually expressed in terms of a 
single survival time value for each person, with possible censoring. 
Andersen and Gill reformulated the same problem as a counting process; 
as time marches onward we observe the events for a subject, rather 
like watching a Geiger counter. 
The data for a subject is presented as multiple rows or &quot;observations&quot;, 
each 
of which applies to an interval of observation (start, stop].
</p>
<p>The routine internally scales and centers data to avoid overflow in
the argument to the exponential function.  These actions do not change
the result, but lead to more numerical stability.
However, arguments to offset are not scaled since there are situations
where a large offset value is a purposefully used.
In general, however, users should not avoid very large numeric values
for an offset due to possible loss of precision in the estimates.
</p>


<h3>Value</h3>

<p>an object of class <code>coxph</code> representing the fit. 
See <code>coxph.object</code> for details.  
</p>


<h3>Side Effects</h3>

<p>Depending on the call, the <code>predict</code>, <code>residuals</code>,
and <code>survfit</code> routines may 
need to reconstruct the x matrix created by <code>coxph</code>.
It is possible for this to fail, as in the example below in
which the predict function is unable to find <code>tform</code>.
</p>
<pre>  tfun &lt;- function(tform) coxph(tform, data=lung)
  fit &lt;- tfun(Surv(time, status) ~ age)
  predict(fit)</pre>
<p>In such a case add the <code>model=TRUE</code> option to the
<code>coxph</code> call to obviate the
need for reconstruction, at the expense of a larger <code>fit</code>
object.  
</p>


<h3>Case weights</h3>

<p>Case weights are treated as replication weights, i.e., a case weight of
2 is equivalent to having 2 copies of that subject's observation.
When computers were much smaller grouping like subjects together was a
common trick to used to conserve memory.  Setting all weights to 2 for
instance will give the same coefficient estimate but halve the variance.
When the Efron approximation for ties (default) is employed replication
of the data will not give exactly the same coefficients as the weights option,
and in this case the weighted fit is arguably the correct one.
</p>
<p>When the model includes a <code>cluster</code> term or the <code>robust=TRUE</code>
option the computed variance treats any weights as sampling weights;
setting all weights to 2 will in this case give the same variance as weights of 1.
</p>


<h3>Special terms</h3>

<p>There are three special terms that may be used in the model equation. 
A <code>strata</code> term identifies a stratified Cox model; separate baseline 
hazard functions are fit for each strata. 
The <code>cluster</code> term is used to compute a robust variance for the model. 
The term <code>+ cluster(id)</code> where each value of <code>id</code> is unique is
equivalent to 
specifying the <code>robust=TRUE</code> argument.
If the <code>id</code> variable is not 
unique, it is assumed that it identifies clusters of correlated
observations.
The robust estimate arises from many different arguments and thus has
had many labels.  It is variously known as the
Huber sandwich estimator, White's estimate (linear models/econometrics),
the Horvitz-Thompson estimate (survey sampling), the working
independence variance (generalized estimating equations), the
infinitesimal jackknife, and the Wei, Lin, Weissfeld (WLW) estimate.
</p>
<p>A time-transform term allows variables to vary dynamically in time.  In
this case the <code>tt</code> argument will be a function or a list of
functions (if there are more than one tt() term in the model) giving the
appropriate transform.   See the examples below.
</p>


<h3>Convergence</h3>

<p>In certain data cases the actual MLE estimate of a 
coefficient is infinity, e.g., a dichotomous variable where one of the 
groups has no events.  When this happens the associated coefficient 
grows at a steady pace and a race condition will exist in the fitting 
routine: either the log likelihood converges, the information matrix 
becomes effectively singular, an argument to exp becomes too large for 
the computer hardware, or the maximum number of interactions is
exceeded.
(Nearly always the first occurs.)
The routine attempts to detect when this has happened,
not always successfully.
The primary consequence for he user is that the Wald statistic =
coefficient/se(coefficient) is not valid in this case and should be
ignored; the likelihood ratio and score tests remain valid however.
</p>


<h3>Ties</h3>

<p>There are three possible choices for handling tied event times.
The Breslow approximation is the easiest to program and hence became the
first option coded for almost all computer routines. It then ended up
as the default option when other options were added in order to &quot;maintain
backwards compatability&quot;.  The Efron option is more accurate if there are
a large number of ties, and it is the default option here.
In practice the number of ties is usually small, in which case all the
methods are statistically indistinguishable.
</p>
<p>Using the &quot;exact partial likelihood&quot; approach the Cox partial likelihood
is equivalent to that for matched logistic regression.  (The
<code>clogit</code> function uses the <code>coxph</code> code to do the fit.)
It is technically appropriate when the time scale is discrete and has
only a few unique values, and some packages refer to this as the
&quot;discrete&quot; option.  There is also an &quot;exact marginal likelihood&quot; due to
Prentice which is not implemented here.
</p>
<p>The calculation of the exact partial likelihood is numerically intense.
Say for instance 180 subjects are at risk on day 7 of which 15 had an event; then
the code needs to compute sums over all 180-choose-15 &gt; 10^43 different
possible subsets of size 15.  There is an efficient recursive algorithm
for this task, but even with this the computation can be insufferably
long.  With (start, stop) data it is much worse since the recursion
needs to start anew for each unique start time.
</p>
<p>A second issue is that of artificial ties due to floating-point
imprecision. See the vignette on this topic for a full explanation or
the <code>timefix</code> option in <code>coxph.control</code>.
Users may need to add <code>timefix=FALSE</code> for simulated data sets.
</p>


<h3>Penalized regression</h3>

<p><code>coxph</code> can now maximise a penalised partial likelihood with
arbitrary user-defined penalty.  Supplied penalty functions include
ridge regression (<a href="ridge.html">ridge</a>), smoothing splines
(<a href="pspline.html">pspline</a>), and frailty models (<a href="frailty.html">frailty</a>).
</p>


<h3>References</h3>

<p>Andersen, P. and Gill, R. (1982). 
Cox's regression model for counting processes, a large sample study. 
<em>Annals of Statistics</em>
<b>10</b>, 1100-1120. 
</p>
<p>Therneau, T., Grambsch, P., Modeling Survival Data: Extending the Cox Model. 
Springer-Verlag, 2000.
</p>


<h3>See Also</h3>

<p><code><a href="coxph.object.html">coxph.object</a></code>, <code><a href="coxph.control.html">coxph.control</a></code>,
<code><a href="cluster.html">cluster</a></code>,  <code><a href="strata.html">strata</a></code>,  <code><a href="Surv.html">Surv</a></code>,
<code><a href="survfit.html">survfit</a></code>, <code><a href="pspline.html">pspline</a></code>, 
<code><a href="ridge.html">ridge</a></code>.   
</p>


<h3>Examples</h3>

<pre>
# Create the simplest test data set 
test1 &lt;- list(time=c(4,3,1,1,2,2,3), 
              status=c(1,1,1,0,1,1,0), 
              x=c(0,2,1,1,1,0,0), 
              sex=c(0,0,0,0,1,1,1)) 
# Fit a stratified model 
coxph(Surv(time, status) ~ x + strata(sex), test1) 
# Create a simple data set for a time-dependent model 
test2 &lt;- list(start=c(1,2,5,2,1,7,3,4,8,8), 
              stop=c(2,3,6,7,8,9,9,9,14,17), 
              event=c(1,1,1,1,1,1,1,0,0,0), 
              x=c(1,0,0,1,0,1,1,1,0,0)) 
summary(coxph(Surv(start, stop, event) ~ x, test2)) 

#
# Create a simple data set for a time-dependent model
#
test2 &lt;- list(start=c(1, 2, 5, 2, 1, 7, 3, 4, 8, 8),
                stop =c(2, 3, 6, 7, 8, 9, 9, 9,14,17),
                event=c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0),
                x    =c(1, 0, 0, 1, 0, 1, 1, 1, 0, 0) )


summary( coxph( Surv(start, stop, event) ~ x, test2))

# Fit a stratified model, clustered on patients 

bladder1 &lt;- bladder[bladder$enum &lt; 5, ] 
coxph(Surv(stop, event) ~ (rx + size + number) * strata(enum) + 
      cluster(id), bladder1)

# Fit a time transform model using current age
coxph(Surv(time, status) ~ ph.ecog + tt(age), data=lung,
     tt=function(x,t,...) pspline(x + t/365.25))
</pre>

<hr /><div style="text-align: center;">[Package <em>survival</em> version 2.42-6 <a href="00Index.html">Index</a>]</div>
</body></html>
